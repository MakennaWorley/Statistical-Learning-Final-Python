{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Introduction",
   "id": "855fdb5aa1a4cf7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this project, I analyze a simulated cohort generated from a coalescent-based genetic model. Each row corresponds to one individual and includes both demographic covariates and genotype-derived predictors suitable for statistical learning.\n",
    "\n",
    "The dataset used in this project was generated using a population-genetic simulation pipeline built with msprime, a coalescent simulator designed for scalable and biologically realistic genomic data. Instead of relying on a pre-existing real-world dataset, this approach creates a fully synthetic cohort in which the true effect sizes, covariates, and sources of noise are known. This allows direct evaluation of how well different statistical learning methods recover the underlying structure of the data.\n",
    "\n",
    "The msprime pipeline produces a tree sequence under a Wright–Fisher coalescent model with recombination. From this tree sequence, a diploid genotype matrix is extracted and filtered by minor allele frequency. A subset of variants is designated as causal, each assigned an effect size drawn from a specified distribution. These effect sizes are used to construct a standardized polygenic score for each simulated individual. In addition to genetic predictors, msprime generates demographic and environmental covariates—sex, age, and an environmental index—to mimic realistic non-genetic influences on phenotype.\n",
    "\n",
    "A continuous quantitative trait (quant_trait) is then created from a linear model combining the polygenic score, covariates, and Gaussian noise. This produces a dataset with controlled polygenic signal, demographic structure, and stochastic variation. Optional principal components (PC1, PC2, …) derived from genotype PCA provide population-structure covariates when needed.\n",
    "\n",
    "Overall, each row of the dataset includes:\n",
    "- Demographic covariates: sex, age, env_index\n",
    "- Genetic predictors: polygenic_score, optional PCA components\n",
    "- Response variable: quant_trait\n",
    "\n",
    "This design makes the dataset ideal for evaluating linear regression, subset selection, and shrinkage methods. Because the true generative parameters are known, the analysis can examine not only predictive performance (RMSE, R²) but also how closely the fitted models recover the true effect sizes used to generate the phenotype. A detailed description of the data-generation process is provided in the project’s msprime documentation.\n",
    "\n",
    "The goals of this project are to use statistical learning methods to:\n",
    "- quantify how much of the variation in the quantitative trait is explained by the polygenic score and covariates,\n",
    "- evaluate whether controlling for population structure via principal components improves prediction,\n",
    "- compare classical linear models, subset selection, and shrinkage methods, and\n",
    "- assess how well the fitted models recover the true underlying effect sizes used during data generation."
   ],
   "id": "ec0cd768ff944299"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports",
   "id": "17962437595a5b7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")"
   ],
   "id": "ee7653eeeb4695f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load the Data",
   "id": "50481243d7e84591"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cohort_path = \"../data/3195663216_msprime_sim_cohort.csv\"\n",
    "effects_path = \"../data/3195663216_msprime_effect_sizes.csv\"\n",
    "\n",
    "cohort = pd.read_csv(cohort_path)\n",
    "effects = pd.read_csv(effects_path)"
   ],
   "id": "b650e44e4844580d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### EDA",
   "id": "9b48cd123884877d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Grab code from exploratory.ipynb",
   "id": "64fe4719713af2d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Classification",
   "id": "c87313714a3d633c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Identify Numerical + Categorical Features",
   "id": "ed431ffb7437658e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numeric_features = [\"age\", \"env_index\", \"polygenic_score\"]\n",
    "\n",
    "categorical_features = [\"sex\"]\n",
    "\n",
    "pcs_features = [col for col in cohort.columns if col.startswith(\"PC\")]\n",
    "\n",
    "numeric_features, categorical_features"
   ],
   "id": "fe655084c5bfa4e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Preparation",
   "id": "c6e3fc40c3bbc2bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = cohort[numeric_features + pcs_features + categorical_features]\n",
    "y = cohort[\"disease_status\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ],
   "id": "4a6c823ff530364d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Standardization",
   "id": "6e670ce3a3e94555"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit *only* on training numerical features\n",
    "X_train_scaled_num = scaler.fit_transform(X_train[numeric_features + pcs_features])\n",
    "X_test_scaled_num = scaler.transform(X_test[numeric_features + pcs_features])\n",
    "\n",
    "# Rebuild full DataFrames with same column order\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    X_train_scaled_num,\n",
    "    columns=numeric_features + pcs_features,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    X_test_scaled_num,\n",
    "    columns=numeric_features + pcs_features,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Add categorical features (sex)\n",
    "X_train_scaled[\"sex\"] = X_train[\"sex\"].values\n",
    "X_test_scaled[\"sex\"] = X_test[\"sex\"].values\n",
    "\n",
    "X_train_scaled.head()"
   ],
   "id": "f0ce4454082b2a19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Matrices",
   "id": "61884eb5a155cf5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "core_vars = numeric_features + pcs_features + categorical_features\n",
    "\n",
    "X_train_full = X_train_scaled[core_vars]\n",
    "X_test_full = X_test_scaled[core_vars]"
   ],
   "id": "6e8f23b1e08a6a25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if pcs:\n",
    "    full_pca_vars = core_vars + pcs\n",
    "    X_train_full_pca = X_train_scaled[full_pca_vars]\n",
    "    X_test_full_pca = X_test_scaled[full_pca_vars]\n",
    "else:\n",
    "    X_train_full_pca = X_train_full.copy()\n",
    "    X_test_full_pca = X_test_full.copy()\n",
    "\n",
    "X_train_full_pca.head()"
   ],
   "id": "4a66a999e62f6420",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train_scaled[numeric_features].mean(), X_train_scaled[numeric_features].std()",
   "id": "e85d01519c1a6c31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The means are almost 0 and the SD are close to 1. No data leakage",
   "id": "36c58c54d0a2aeb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_test_scaled[numeric_features].mean(), X_test_scaled[numeric_features].std()",
   "id": "41b14fb417f6b479",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Classification Models",
   "id": "2eeab2be7ff957ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "def evaluate_classifier(model, name):\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_prob = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, \"predict_proba\") else model.decision_function(X_test_scaled)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": acc,\n",
    "        \"auc\": auc\n",
    "    }"
   ],
   "id": "d3ffc0d20b10e9ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "results_clf = []\n",
    "\n",
    "results_clf.append(\n",
    "    evaluate_classifier(LogisticRegression(max_iter=5000), \"Logistic Regression\")\n",
    ")\n",
    "\n",
    "results_clf.append(\n",
    "    evaluate_classifier(LinearDiscriminantAnalysis(), \"LDA\")\n",
    ")\n",
    "\n",
    "results_clf.append(\n",
    "    evaluate_classifier(QuadraticDiscriminantAnalysis(), \"QDA\")\n",
    ")\n",
    "\n",
    "results_clf.append(\n",
    "    evaluate_classifier(KNeighborsClassifier(n_neighbors=11), \"KNN (k=11)\")\n",
    ")\n",
    "\n",
    "results_clf.append(\n",
    "    evaluate_classifier(SVC(kernel=\"rbf\", probability=True), \"SVM (RBF)\")\n",
    ")\n",
    "\n",
    "pd.DataFrame(results_clf)"
   ],
   "id": "32adf505ba2cfcc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Five classification models were evaluated using `disease_status` as the binary response: logistic regression, LDA, QDA, KNN (k=11), and SVM with an RBF kernel. Logistic regression and LDA achieved the best overall performance, each obtaining an accuracy of approximately 68.6% and an ROC AUC of about 0.758. This is consistent with the fact that the disease phenotype was generated from a linear logistic function of the predictors, making the true decision boundary nearly linear.\n",
    "\n",
    "QDA performed slightly worse (accuracy ≈ 68.3%, AUC ≈ 0.749), likely due to the added variance introduced by estimating separate covariance matrices for each class, which the data do not strongly support. SVM with an RBF kernel performed reasonably well (accuracy ≈ 67.9%, AUC ≈ 0.745), but its nonlinear flexibility did not improve performance because the underlying relationship is fundamentally linear. KNN achieved the lowest performance (accuracy ≈ 64.4%, AUC ≈ 0.707), which is expected given that KNN struggles with continuous, weakly separated data and does not exploit the linear structure present in the true model.\n",
    "\n",
    "Overall, the classification task demonstrates moderate predictive ability, reflecting the limited signal in the disease phenotype relative to the quantitative trait. This task primarily serves as the mechanical portion of the assignment, while the regression analysis provides the more scientifically meaningful insights.\n"
   ],
   "id": "c3de65aaa6f11713"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=5000)\n",
    "log_reg.fit(X_train_scaled, y_train)  # <-- Fit once\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "RocCurveDisplay.from_estimator(log_reg, X_test_scaled, y_test)\n",
    "plt.title(\"ROC Curve — Logistic Regression\")\n",
    "plt.show()\n"
   ],
   "id": "f60528892b160a7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The ROC curve for logistic regression shows moderate classification performance with an AUC of approximately 0.76. This is consistent with the data-generating process, where the disease phenotype was simulated as a noisy logistic function of the polygenic score and environmental exposure. Because the model form matches the true underlying logistic relationship, the ROC curve is smooth and convex, demonstrating effective discrimination between the two classes. However, the intentional randomness in the Bernoulli sampling step limits the maximum achievable AUC, explaining why the classifier does not approach perfect separation. Overall, logistic regression performs well for this mechanical classification task, but the binary disease trait contains substantially less signal than the quantitative trait analyzed in the regression section.",
   "id": "17fec432ca1e4ce6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=5000).fit(X_train_scaled, y_train)\n",
    "ConfusionMatrixDisplay.from_estimator(log_reg, X_test_scaled, y_test)\n",
    "plt.title(\"Confusion Matrix — Logistic Regression\")\n",
    "plt.show()"
   ],
   "id": "2b8ecd4d3c03eb6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The confusion matrix reflects moderate but meaningful classification performance. Logistic regression correctly identifies 1035 healthy individuals (true negatives) and 1024 diseased individuals (true positives). Misclassifications occur in both directions, with 590 false positives and 441 false negatives. The number of false negatives is lower than false positives, indicating the model is somewhat more sensitive than specific at the default 0.5 threshold.\n",
    "\n",
    "These results are fully consistent with the data-generating process used in the simulation. The disease phenotype was generated as a noisy logistic transformation of the polygenic score and environmental index, so perfect separation is not possible. The presence of 441 false negatives and 590 false positives reflects the inherent stochasticity in the Bernoulli sampling step and the moderate effect sizes of the predictors. Combined with the ROC AUC value of approximately 0.76, the confusion matrix confirms that logistic regression provides reasonable discrimination between cases and controls while still respecting the limitations imposed by the simulation's noise structure."
   ],
   "id": "f372f045595bbf36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Regression",
   "id": "8b81dd9491b0f0d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Identify Numerical + Categorical Features",
   "id": "5cd1960aae7d24ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numeric_features = [\"age\", \"env_index\", \"polygenic_score\"]\n",
    "\n",
    "categorical_features = [\"sex\"]\n",
    "\n",
    "pcs_features = [col for col in cohort.columns if col.startswith(\"PC\")]\n",
    "\n",
    "numeric_features, categorical_features"
   ],
   "id": "6c7d52a1e6a8ecf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Preparation",
   "id": "6db4d27e7dec1b23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = cohort[numeric_features + pcs_features + categorical_features]\n",
    "y = cohort[\"quant_trait\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ],
   "id": "7ee715efc144aae0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Standardization",
   "id": "d19d1abdf1e93860"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit *only* on training numerical features\n",
    "X_train_scaled_num = scaler.fit_transform(X_train[numeric_features + pcs_features])\n",
    "X_test_scaled_num = scaler.transform(X_test[numeric_features + pcs_features])\n",
    "\n",
    "# Rebuild full DataFrames with same column order\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    X_train_scaled_num,\n",
    "    columns=numeric_features + pcs_features,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    X_test_scaled_num,\n",
    "    columns=numeric_features + pcs_features,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Add categorical features (sex)\n",
    "X_train_scaled[\"sex\"] = X_train[\"sex\"].values\n",
    "X_test_scaled[\"sex\"] = X_test[\"sex\"].values\n",
    "\n",
    "X_train_scaled.head()"
   ],
   "id": "1eb5c8def79f439e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Matrices",
   "id": "f33a6e9bb270f63e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) Baseline: PRS only\n",
    "features_baseline = [\"polygenic_score\"]\n",
    "\n",
    "# 2) Full model: PRS + covariates (NO PCs)\n",
    "features_full = [\"age\", \"env_index\", \"polygenic_score\", \"sex\"]\n",
    "\n",
    "# 3) Full + PCA: PRS + covariates + PCs\n",
    "features_full_pca = features_full + pcs_features"
   ],
   "id": "57ef3b37757e1155",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_baseline = X_train_scaled[features_baseline]\n",
    "X_test_baseline = X_test_scaled[features_baseline]\n",
    "\n",
    "X_train_baseline.head(), X_train_baseline.mean(), X_train_baseline.std()"
   ],
   "id": "ca920f10e48aa165",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_full = X_train_scaled[features_full]\n",
    "X_test_full = X_test_scaled[features_full]\n",
    "\n",
    "X_train_full.head(), X_train_full.mean(), X_train_full.std()"
   ],
   "id": "f4bb463f487c824b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_full_pca = X_train_scaled[features_full_pca]\n",
    "X_test_full_pca  = X_test_scaled[features_full_pca]\n",
    "\n",
    "X_train_full_pca.head(), X_train_full_pca.mean(), X_train_full_pca.std()"
   ],
   "id": "d0fa13b67a0609a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Baseline Linear Models",
   "id": "cd084b3f929fbc66"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Simple Linear Regression (quant_trait ~ PRS)",
   "id": "c282c9fc554b9bb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"rmse_train\": rmse_train,\n",
    "        \"rmse_test\": rmse_test,\n",
    "        \"r2_train\": r2_train,\n",
    "        \"r2_test\": r2_test,\n",
    "    }\n",
    "\n",
    "def plot_pred_vs_true(model, X_test, y_test, label):\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "\n",
    "    # Regression line\n",
    "    slope, intercept = np.polyfit(y_test, y_pred, 1)\n",
    "    plt.plot(y_test, slope*y_test + intercept, color=\"blue\", label=\"Regression Line\")\n",
    "\n",
    "    # Identity line\n",
    "    plt.plot([y_test.min(), y_test.max()],\n",
    "             [y_test.min(), y_test.max()],\n",
    "             color=\"red\", linestyle=\"--\", label=\"Identity Line\")\n",
    "\n",
    "    plt.xlabel(\"True Quantitative Trait\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(f\"Predicted vs True ({label})\")\n",
    "\n",
    "    # R² annotation\n",
    "    plt.text(0.05, 0.9, f\"$R^2 = {r2:.3f}$\",\n",
    "             transform=plt.gca().transAxes,\n",
    "             fontsize=12, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_residual_hist(model, X_test, y_test, label):\n",
    "    y_pred = model.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(residuals, kde=True, bins=30)\n",
    "    plt.title(f\"Residual Distribution ({label})\")\n",
    "    plt.xlabel(\"Residual\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_residuals_vs_fitted(model, X_test, y_test, label):\n",
    "    y_pred = model.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "    plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Fitted Values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(f\"Residuals vs Fitted ({label})\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_qq(model, X_test, y_test, label):\n",
    "    y_pred = model.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(f\"QQ Plot of Residuals ({label})\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_full_diagnostics(model, X_test, y_test, label):\n",
    "    y_pred = model.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12,10))\n",
    "\n",
    "    # Predicted vs True\n",
    "    axs[0,0].scatter(y_test, y_pred, alpha=0.5)\n",
    "    slope, intercept = np.polyfit(y_test, y_pred, 1)\n",
    "    axs[0,0].plot(y_test, slope*y_test + intercept, color=\"blue\")\n",
    "    axs[0,0].plot([y_test.min(), y_test.max()],\n",
    "                  [y_test.min(), y_test.max()],\n",
    "                  color=\"red\", linestyle=\"--\")\n",
    "    axs[0,0].set_title(f\"Predicted vs True ({label})\\n$R^2 = {r2:.3f}$\")\n",
    "\n",
    "    # Residual Histogram\n",
    "    sns.histplot(residuals, kde=True, bins=20, ax=axs[0,1])\n",
    "    axs[0,1].set_title(\"Residual Distribution\")\n",
    "\n",
    "    # Residuals vs Fitted\n",
    "    axs[1,0].scatter(y_pred, residuals, alpha=0.5)\n",
    "    axs[1,0].axhline(0, color=\"red\", linestyle=\"--\")\n",
    "    axs[1,0].set_title(\"Residuals vs Fitted\")\n",
    "    axs[1,0].set_xlabel(\"Fitted\")\n",
    "    axs[1,0].set_ylabel(\"Residuals\")\n",
    "\n",
    "    # QQ Plot\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axs[1,1])\n",
    "    axs[1,1].set_title(\"QQ Plot\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "294e925fd0ff94a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1. Baseline: PRS only\n",
    "lr_baseline = LinearRegression().fit(X_train_baseline, y_train)\n",
    "results.append(\n",
    "    evaluate_model(lr_baseline, X_train_baseline, y_train, X_test_baseline, y_test, \"Baseline (PRS only)\")\n",
    ")\n",
    "\n",
    "# 2. Full model\n",
    "lr_full = LinearRegression().fit(X_train_full, y_train)\n",
    "results.append(\n",
    "    evaluate_model(lr_full, X_train_full, y_train, X_test_full, y_test, \"Full (PRS + covariates)\")\n",
    ")\n",
    "\n",
    "# 3. Full + PCA\n",
    "lr_full_pca = LinearRegression().fit(X_train_full_pca, y_train)\n",
    "results.append(\n",
    "    evaluate_model(lr_full_pca, X_train_full_pca, y_train, X_test_full_pca, y_test, \"Full + PCA\")\n",
    ")\n",
    "\n",
    "results_cohort = pd.DataFrame(results)\n",
    "results_cohort"
   ],
   "id": "415f9f7ae25d4c41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Baseline (PRS only)\n",
    "The PRS-only model explains ~44% of variance in training and ~44% in testing (r² ≈ 0.44). This indicates that the simulated trait is substantially but not exclusively genetically driven, consistent with a moderately polygenic trait where environmental effects remain important. The relatively high RMSE also reflects the missing non-genetic signal.\n",
    "\n",
    "#### Full Model (PRS + covariates)\n",
    "Adding environmental and demographic covariates improves performance to r² ≈ 0.56 and lowers RMSE on both train and test sets. This is a meaningful gain over the baseline, showing that the simulation indeed encoded environmental variation that is predictive of the phenotype. The identical train/test performance also indicates a well-specified model without overfitting.\n",
    "\n",
    "#### Full + PCA\n",
    "Including ancestry PCs does not change performance (train/test r² remain 0.566). This suggests that population structure does not meaningfully confound or predict the trait in your simulated dataset. Because the simulation likely didn’t tie trait values to subpopulation ancestry, the PCs add no new information."
   ],
   "id": "f12172267f6acd05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_pred_vs_true(\n",
    "    model=lr_full_pca,\n",
    "    X_test=X_test_full_pca,\n",
    "    y_test=y_test,\n",
    "    label=\"Full + PCA Model\"\n",
    ")"
   ],
   "id": "d13a9f80682c17f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Predicted vs. True (Full + PCA Model)\n",
    "\n",
    "The model captures moderate-to-strong linear signal (R² ≈ 0.567), which aligns with the earlier table: the trait is influenced by both genetic and environmental inputs, and the full model successfully recovers a little over half of the variance.\n",
    "\n",
    "However, the slope of the regression line is shallower than the identity line, which indicates regression toward the mean — a common pattern in polygenic prediction. Extreme trait values (both high and low) tend to be under-predicted, visible in the flattening of the cloud and the blue line below the dashed identity line at the tails.\n",
    "\n",
    "This pattern is expected when:\n",
    "- the trait has non-genetic variation the model cannot fully capture\n",
    "- the PRS has modest accuracy rather than very high signal\n",
    "- random environmental noise is present\n",
    "- population structure (PCA) adds no additional predictive value, so the model’s information ceiling doesn’t increase\n",
    "\n",
    "The tight clustering around the regression line, with symmetric scatter, shows that the model is well-calibrated and not overfitting—train and test r² match almost perfectly."
   ],
   "id": "88b20e477e04ed27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_residual_hist(\n",
    "    model=lr_full_pca,\n",
    "    X_test=X_test_full_pca,\n",
    "    y_test=y_test,\n",
    "    label=\"Full + PCA Model\"\n",
    ")"
   ],
   "id": "2d86cf148974d775",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Residual Distribution (Full + PCA Model)\n",
    "\n",
    "The residuals are approximately symmetric and bell-shaped, centered tightly around zero. This indicates that the model’s errors are:\n",
    "- unbiased (no systematic over- or under-prediction)\n",
    "- homoscedastic (variance is roughly constant across the range of predicted values)\n",
    "- approximately Gaussian, which is exactly what linear regression assumes\n",
    "\n",
    "The slight elongation in the right tail is expected for quantitative traits with environmental noise—occasionally, individuals have higher trait values that the model cannot fully capture given limited genetic + covariate information.\n",
    "\n",
    "Importantly, there are no multimodal patterns or extreme skew, which means:\n",
    "- no hidden substructure affecting the trait\n",
    "- adding PCA (ancestry) was appropriately unnecessary\n",
    "- the full model is well-specified\n",
    "- environmental noise, not model misspecification, is the primary driver of error\n",
    "\n",
    "This residual pattern is exactly what you'd expect for a simulated trait with moderate genetic architecture and additive environmental variance."
   ],
   "id": "5c0b56b84dfd569d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_residuals_vs_fitted(\n",
    "    model=lr_full_pca,\n",
    "    X_test=X_test_full_pca,\n",
    "    y_test=y_test,\n",
    "    label=\"Full + PCA Model\"\n",
    ")"
   ],
   "id": "e195d2dcec824195",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Residuals vs. Fitted (Full + PCA Model)\n",
    "\n",
    "The residuals show a uniform horizontal band around zero across the entire range of fitted values. This is exactly what you want to see for a correctly specified linear model. Specifically:\n",
    "- No visible pattern, curvature, or funnel shape → suggests the relationship between predictors and the trait is well-captured by a linear additive model.\n",
    "- Constant vertical spread → indicates homoscedasticity; the variance of errors does not increase at higher or lower predicted values.\n",
    "- No clusters or stripes → confirms that there are no hidden subgroups, unmodeled interactions, or leftover population structure affecting the trait.\n",
    "- No systematic drift above or below the zero line → shows unbiased predictions (the model does not systematically over- or underpredict across the range).\n",
    "\n",
    "Together, this plot supports your earlier results: the simulation produced a trait that is linear, additive, and not confounded by ancestry, and the full model (PRS + covariates) is appropriately specified and performing as expected."
   ],
   "id": "34967a396171bde5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_qq(\n",
    "    model=lr_full_pca,\n",
    "    X_test=X_test_full_pca,\n",
    "    y_test=y_test,\n",
    "    label=\"Full + PCA Model\"\n",
    ")"
   ],
   "id": "949a567747f1a340",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### QQ Plot of Residuals (Full + PCA Model)\n",
    "\n",
    "The QQ plot shows that the residuals fall almost perfectly along the 45° reference line, indicating that:\n",
    "- the residuals are very close to normally distributed\n",
    "- the linear model’s assumption of normality is well satisfied\n",
    "- there is no strong skew or kurtosis in the error distribution\n",
    "- the simulation did not introduce unusual heavy tails or outlier structure\n",
    "\n",
    "The slight upward deviation in the uppermost tail (the very top-right few points) is mild and expected for quantitative traits with some environmental noise—extreme observations are inherently harder to predict, but this deviation is not large enough to indicate a problem.\n",
    "\n",
    "Overall, the QQ plot confirms that:\n",
    "- the model is appropriately specified\n",
    "- the residual variance structure is compatible with linear regression assumptions\n",
    "- no transformation or re-specification of the trait is necessary\n",
    "\n",
    "Combined with the histogram and residuals-vs-fitted plots, this provides strong evidence that the Full + PCA model satisfies all core linear regression diagnostics."
   ],
   "id": "16464da5bbf94f89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Simple Linear Regression (quant_trait ~ PRS): Model Overview\n",
    "\n",
    "The baseline model using only the polygenic score explains approximately 44% of the phenotypic variance (R² ≈ 0.44), which aligns with the simulation design where the PRS carries the largest effect (β = 1.0). The corresponding RMSE remains relatively high, indicating that the purely genetic model captures substantial signal but leaves environmental variation unexplained. This behavior is expected for a moderately heritable polygenic trait with additional non-genetic inputs.\n",
    "\n",
    "Expanding the model to include covariates (sex, age, and the environmental index) yields a marked improvement in performance. The R² increases to ~0.57 with a corresponding decrease in RMSE for both the training and test partitions. This improvement is consistent with the data-generating process, in which the environmental index (β = 0.5) and sex (β = 0.3) contribute meaningful additive effects to the quantitative trait. The identical performance on training and testing sets further indicates that the model is not overfitting, but instead capturing genuine structure present in the simulation.\n",
    "\n",
    "Incorporating principal components has no measurable effect on predictive accuracy, with R² and RMSE values identical to the Full model. This outcome indicates that population structure does not influence the trait, which is consistent with the msprime coalescent simulation: the dataset represents a single panmictic population without substructure, and the phenotype was generated independently of ancestry. Thus, the PCs provide no additional explanatory information.\n",
    "\n",
    "Across all diagnostic plots, the residual behavior confirms that the linear modeling assumptions are well satisfied. The predicted-versus-true scatterplot displays a strong linear relationship with mild regression to the mean at phenotypic extremes, reflecting inherent environmental noise and the limits of PRS-based prediction. The residual histogram is approximately Gaussian and centered around zero, and the QQ plot shows that deviations from normality are minimal and restricted to the upper tail. The residuals-versus-fitted plot demonstrates homoscedasticity and the absence of nonlinear patterns or hidden substructure. Together, these diagnostics indicate that the model is well-calibrated, unbiased, and correctly specified.\n",
    "\n",
    "Overall, the Full + PCA model provides a reliable baseline for the quantitative trait, successfully recovering the genetic and environmental signals encoded in the simulation. These results establish a strong foundation for comparison with model selection, regularization, and shrinkage-based approaches in subsequent analyses."
   ],
   "id": "d4f092fc8143611a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Multiple Linear Regression ( sex + age + env_index + PRS )",
   "id": "c50e941b0d35f2ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mlr_features = [\"sex\", \"age\", \"env_index\", \"polygenic_score\"]\n",
    "X_train_mlr = X_train_scaled[mlr_features]\n",
    "X_test_mlr  = X_test_scaled[mlr_features]"
   ],
   "id": "a4f8ff3b7cc172e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = []\n",
    "\n",
    "# 1. Multiple Linear Regression (PRS + sex + age + env_index)\n",
    "lr_mlr = LinearRegression().fit(X_train_mlr, y_train)\n",
    "results.append(\n",
    "    evaluate_model(\n",
    "        lr_mlr,\n",
    "        X_train_mlr, y_train,\n",
    "        X_test_mlr, y_test,\n",
    "        \"MLR (sex + age + env_index + PRS)\"\n",
    "    )\n",
    ")\n",
    "\n",
    "results_cohort = pd.DataFrame(results)\n",
    "results_cohort"
   ],
   "id": "88c2ce9121d58f95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_full_diagnostics(lr_mlr, X_test_mlr, y_test, \"MLR (sex + age + env_index + PRS)\")",
   "id": "1a38ff5185a8ce47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Multiple Linear Regression (sex + age + env_index + PRS): Model Overview\n",
    "\n",
    "The multiple linear regression model that includes sex, age, the environmental index, and the polygenic score achieves a substantial improvement over the PRS-only baseline. The model explains approximately 56–57% of the variance in both the training and testing sets (R² ≈ 0.566), representing a 12–13 percentage point increase relative to the baseline model. This gain aligns closely with the data-generating process, in which the environmental index (β = 0.5) and sex (β = 0.3) contribute meaningful additive effects alongside the PRS (β = 1.0). The reduction in RMSE further reflects the improved predictive accuracy provided by including these covariates.\n",
    "\n",
    "The predicted-versus-true scatterplot shows a strong linear pattern, with the fitted regression line tracking the identity line closely. Mild regression-to-the-mean at the upper and lower ends of the trait distribution is expected for polygenic traits with environmental noise. The residual distribution is approximately Gaussian and centered around zero, consistent with a well-specified linear model. The QQ plot shows that residuals follow the theoretical normal distribution closely, with only minor upward deviation in the extreme upper tail—an acceptable and typical feature of quantitative trait predictions.\n",
    "\n",
    "The residuals-versus-fitted plot displays a uniform horizontal band with constant variance, indicating homoscedasticity and the absence of nonlinear structure or systematic bias. There is no evidence of heteroscedasticity, curvature, or clustering, supporting the conclusion that the linear-additive form is appropriate for this trait. Importantly, the near-identical train and test statistics indicate extremely stable generalization, confirming that the model is capturing true underlying signal rather than overfitting noise.\n",
    "\n",
    "Together, these diagnostics demonstrate that the multiple linear regression model accurately recovers the genetic and environmental components of the simulated trait, respects the assumptions of linear modeling, and serves as a strong and reliable predictive baseline for the analyses that follow."
   ],
   "id": "659148eb71783f64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Linear Model with Population Structure ( sex + age + env_index + PRS + PC1 + PC2)",
   "id": "530d5c2ca17cc4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pop_features = [\"sex\", \"age\", \"env_index\", \"polygenic_score\", \"PC1\", \"PC2\"]\n",
    "X_train_pop = X_train_scaled[pop_features]\n",
    "X_test_pop  = X_test_scaled[pop_features]"
   ],
   "id": "abefa0b38bfb6fc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = []\n",
    "\n",
    "# 1. Linear Model with Population Structure (sex + age + env_index + PRS + PC1 + PC2)\n",
    "lr_pop = LinearRegression().fit(X_train_pop, y_train)\n",
    "results.append(\n",
    "    evaluate_model(\n",
    "        lr_pop,\n",
    "        X_train_pop, y_train,\n",
    "        X_test_pop, y_test,\n",
    "        \"Population Structure ( sex + age + env_index + PRS + PC1 + PC2)\"\n",
    "    )\n",
    ")\n",
    "\n",
    "results_cohort = pd.DataFrame(results)\n",
    "results_cohort"
   ],
   "id": "c48b47dd3cdfdcb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_full_diagnostics(lr_pop, X_test_pop, y_test, \"Population Structure Model\")",
   "id": "5ee9af41cbf8a0e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Population Structure Model (sex + age + env_index + PRS + PCA): Model Overview\n",
    "\n",
    "The population structure–adjusted model yields identical performance to the standard multiple linear regression model, explaining approximately 56–57% of the variance (R² ≈ 0.566) with matching RMSE values across both training and test sets. The complete overlap in performance indicates that the principal components contribute no additional predictive information beyond the existing covariates and the polygenic score. This result is consistent with the simulation design: the msprime coalescent model generated individuals from a single panmictic population, and the quantitative trait was constructed independently of ancestry, meaning that population structure does not influence the phenotype.\n",
    "\n",
    "The predicted-versus-true scatterplot exhibits a clear linear trend, nearly identical to the model without PCs, with the fitted regression line capturing the central trend and showing mild regression toward the mean at trait extremes. The residual distribution remains approximately Gaussian and centered at zero, further confirming that the inclusion of PCs does not alter model behavior or residual structure. The QQ plot continues to show close agreement with the theoretical normal distribution, with only slight deviation in the upper tail, which is typical for quantitative traits with environmental noise.\n",
    "\n",
    "The residuals-versus-fitted plot again forms a uniform horizontal band with constant variance, indicating homoscedasticity and the absence of nonlinear patterns or hidden structure. Notably, there is no clustering, stratification, or curvature that would suggest unmodeled ancestry effects—reinforcing the conclusion that the principal components are statistically irrelevant for this particular trait.\n",
    "\n",
    "Overall, the population structure model validates that ancestry does not confound, predict, or interact with the simulated quantitative trait. The linear model remains well-specified, unbiased, and stable. These findings confirm that the observed genetic and environmental effects are genuine and not artifacts of population stratification, strengthening the interpretability of subsequent model comparisons and feature selection analyses."
   ],
   "id": "81dd19100aceb5b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Subset Selection",
   "id": "fcea8fb082719ed6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_fs = X_train_full.copy()\n",
    "y_fs = y_train.copy()"
   ],
   "id": "a59276ee5c6f29b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def forward_selection(X, y, criterion=\"aic\"):\n",
    "    remaining = list(X.columns)\n",
    "    selected = []\n",
    "    current_score, best_new_score = np.inf, np.inf\n",
    "\n",
    "    while remaining:\n",
    "        scores_with_candidates = []\n",
    "\n",
    "        for candidate in remaining:\n",
    "            formula_vars = selected + [candidate]\n",
    "            X_model = sm.add_constant(X[formula_vars])\n",
    "            model = sm.OLS(y, X_model).fit()\n",
    "\n",
    "            if criterion == \"aic\":\n",
    "                score = model.aic\n",
    "            else:\n",
    "                score = model.bic\n",
    "\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, best_candidate = scores_with_candidates[0]\n",
    "\n",
    "        if best_new_score < current_score:\n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_score = best_new_score\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return selected, current_score\n",
    "\n",
    "def backward_selection(X, y, criterion=\"aic\"):\n",
    "    selected = list(X.columns)\n",
    "    current_score = np.inf\n",
    "\n",
    "    while len(selected) > 0:\n",
    "        scores_with_candidates = []\n",
    "\n",
    "        for candidate in selected:\n",
    "            trial = selected.copy()\n",
    "            trial.remove(candidate)\n",
    "\n",
    "            X_model = sm.add_constant(X[trial])\n",
    "            model = sm.OLS(y, X_model).fit()\n",
    "\n",
    "            if criterion == \"aic\":\n",
    "                score = model.aic\n",
    "            else:\n",
    "                score = model.bic\n",
    "\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, worst_candidate = scores_with_candidates[0]\n",
    "\n",
    "        if best_new_score < current_score:\n",
    "            selected.remove(worst_candidate)\n",
    "            current_score = best_new_score\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return selected, current_score"
   ],
   "id": "f341ffec3c258227",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "forward_aic_vars, forward_aic_score = forward_selection(X_fs, y_fs, criterion=\"aic\")\n",
    "backward_aic_vars, backward_aic_score = backward_selection(X_fs, y_fs, criterion=\"aic\")\n",
    "\n",
    "forward_aic_vars, backward_aic_vars"
   ],
   "id": "a45ceea82c13b2f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "forward_bic_vars, forward_bic_score = forward_selection(X_fs, y_fs, criterion=\"bic\")\n",
    "backward_bic_vars, backward_bic_score = backward_selection(X_fs, y_fs, criterion=\"bic\")\n",
    "\n",
    "forward_bic_vars, backward_bic_vars"
   ],
   "id": "47da155c777e073e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_subset = []\n",
    "\n",
    "# Fit forward AIC model\n",
    "lr_fwd = LinearRegression().fit(X_train_full[forward_aic_vars], y_train)\n",
    "results_subset.append(\n",
    "    evaluate_model(lr_fwd,\n",
    "                   X_train_full[forward_aic_vars], y_train,\n",
    "                   X_test_full[forward_aic_vars], y_test,\n",
    "                   f\"Forward AIC: {forward_aic_vars}\")\n",
    ")\n",
    "\n",
    "# Fit backward AIC model\n",
    "lr_bwd = LinearRegression().fit(X_train_full[backward_aic_vars], y_train)\n",
    "results_subset.append(\n",
    "    evaluate_model(lr_bwd,\n",
    "                   X_train_full[backward_aic_vars], y_train,\n",
    "                   X_test_full[backward_aic_vars], y_test,\n",
    "                   f\"Backward AIC: {backward_aic_vars}\")\n",
    ")\n",
    "\n",
    "results_subset_df = pd.DataFrame(results_subset)\n",
    "results_subset_df"
   ],
   "id": "61f93c8333237e79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interpretation of Subset Selection Results\n",
    "\n",
    "Both forward and backward stepwise selection (under AIC and BIC) selected the same set of predictors: `polygenic_score`, `env_index`, and `sex`. This matches the true data-generating model used in the simulation, where the polygenic score has the largest effect size (β = 1.0), followed by the environmental index (β = 0.5) and sex (β = 0.3).\n",
    "\n",
    "Notably, `age` was not selected by either AIC or BIC. This is expected because its true coefficient in the simulation (β ≈ 0.02) is extremely small relative to the noise term, making it too weak to meaningfully improve the likelihood once model complexity penalties are applied.\n",
    "\n",
    "The subset-selected model achieves nearly identical performance to the full model (RMSE ≈ 0.64, R² ≈ 0.57), confirming that the excluded variable (`age`) contributes very little predictive signal. This result illustrates that stepwise methods can successfully recover the core structure of the underlying genetic architecture while discarding predictors with minimal effect sizes."
   ],
   "id": "9b5e8317a8e73b7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Shrinkage Methods (Ridge, Lasso, Elastic Net)",
   "id": "de68f578af58911f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "alphas = np.logspace(-4, 4, 200)\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=alphas)\n",
    "ridge_cv.fit(X_train_full, y_train)\n",
    "\n",
    "ridge_alpha = ridge_cv.alpha_\n",
    "ridge_alpha"
   ],
   "id": "1a39bca17955e37c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_ridge = evaluate_model(\n",
    "    ridge_cv,\n",
    "    X_train_full, y_train,\n",
    "    X_test_full, y_test,\n",
    "    f\"Ridge (alpha={ridge_alpha:.4f})\"\n",
    ")"
   ],
   "id": "42291bc7dc9393a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso_cv = LassoCV(\n",
    "    cv=10,\n",
    "    max_iter=5000,\n",
    "    random_state=42\n",
    ")\n",
    "lasso_cv.fit(X_train_full, y_train)\n",
    "\n",
    "lasso_alpha = lasso_cv.alpha_\n",
    "lasso_alpha"
   ],
   "id": "6dfa9247396c426a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_lasso = evaluate_model(\n",
    "    lasso_cv,\n",
    "    X_train_full, y_train,\n",
    "    X_test_full, y_test,\n",
    "    f\"Lasso (alpha={lasso_alpha:.4f})\"\n",
    ")"
   ],
   "id": "30f6dcf7de1f9288",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "elastic_cv = ElasticNetCV(\n",
    "    l1_ratio=np.linspace(0.01, 0.99, 20),\n",
    "    cv=10,\n",
    "    max_iter=5000,\n",
    "    random_state=42\n",
    ")\n",
    "elastic_cv.fit(X_train_full, y_train)\n",
    "\n",
    "elastic_alpha = elastic_cv.alpha_\n",
    "elastic_l1 = elastic_cv.l1_ratio_\n",
    "elastic_alpha, elastic_l1"
   ],
   "id": "fa5166defb4bf84e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_elastic = evaluate_model(\n",
    "    elastic_cv,\n",
    "    X_train_full, y_train,\n",
    "    X_test_full, y_test,\n",
    "    f\"ElasticNet (alpha={elastic_alpha:.4f}, l1_ratio={elastic_l1:.2f})\"\n",
    ")"
   ],
   "id": "8e7e84b0656ee8f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "shrinkage_df = pd.DataFrame([\n",
    "    results_ridge,\n",
    "    results_lasso,\n",
    "    results_elastic\n",
    "])\n",
    "shrinkage_df"
   ],
   "id": "5728ff835824a522",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    \"OLS\": lr_full.coef_,\n",
    "    \"Ridge\": ridge_cv.coef_,\n",
    "    \"Lasso\": lasso_cv.coef_,\n",
    "    \"ElasticNet\": elastic_cv.coef_\n",
    "}, index=X_train_full.columns)\n",
    "\n",
    "coef_df"
   ],
   "id": "94fd3a32c34dad3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "coef_df.plot(kind=\"bar\")\n",
    "plt.title(\"Coefficient Estimates Across Methods\")\n",
    "plt.ylabel(\"Coefficient Value\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "id": "9e56f421eded8b58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Shrinkage Models (Ridge, Lasso, Elastic Net): Model Overview\n",
    "\n",
    "The three shrinkage models—Ridge (α ≈ 4.61), Lasso (α ≈ 0.0007), and Elastic Net (α ≈ 0.0007, l1_ratio = 0.99)—all produced nearly identical predictive performance, achieving test RMSE values around 0.644–0.645 and R² values of approximately 0.566. This performance is indistinguishable from the standard multiple linear regression model, indicating that the dataset is low-dimensional, well-behaved, and free from substantial multicollinearity. In such settings, regularization does not meaningfully alter out-of-sample prediction error because the underlying signal is strong, linear, and correctly specified. The similarity in training and test performance across all penalized models further demonstrates that the quantitative trait is not prone to overfitting under this feature set.\n",
    "\n",
    "Despite the nearly identical predictive performance, the coefficient estimates offer important insights. All three shrinkage methods preserve the hierarchical importance of predictors: polygenic_score remains the dominant predictor with coefficients around 0.656, env_index retains a moderate effect (\\~0.345), and sex maintains a smaller but clearly nonzero contribution (~0.186). These estimates align closely with the true generative architecture of the phenotype, where the polygenic score had the largest true effect (β = 1.0), followed by environmental exposure (β = 0.5) and sex (β = 0.3) before standardization. The consistency of these coefficients across OLS, Ridge, Lasso, and Elastic Net reinforces the robustness of the linear model in capturing the major sources of trait variation.\n",
    "\n",
    "The most notable shrinkage behavior appears in the treatment of age, whose estimated coefficient is extremely small across all models (≈0.009). This matches the simulation design, where age was assigned a very weak true effect (β ≈ 0.02). Lasso and Elastic Net apply slightly stronger penalization to this parameter, pulling it marginally closer to zero than Ridge and OLS. The fact that Lasso does not fully zero out the coefficient reflects that age does have a weak but consistent signal, albeit one too small to meaningfully influence prediction or justify inclusion in subset selection models. This subtle shrinkage is exactly the behavior expected in a correctly specified linear system where one predictor contributes minimally relative to noise.\n",
    "\n",
    "Taken together, the shrinkage models confirm that the quantitative trait is driven primarily by the polygenic score and environmental exposure, with sex contributing a moderate secondary signal and age contributing only marginally. The near-identical performance across penalized and unpenalized models demonstrates that regularization is not necessary for prediction in this setting but still provides valuable interpretability regarding coefficient stability and relative effect sizes. These results align perfectly with the known generative process of the dataset and further validate the conclusions drawn from the subset selection analyses."
   ],
   "id": "3ac75a6b48a9629c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linking Model Results to the True Genetic Architecture",
   "id": "d7f49b9c8b61175b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "beta_age_true = 0.02\n",
    "beta_env_true = 0.5\n",
    "beta_prs_true = 1.0\n",
    "beta_sex_true = 0.3"
   ],
   "id": "dcc6d617392c686e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "coef_estimates = pd.DataFrame({\n",
    "    \"OLS\": lr_full.coef_,\n",
    "    \"Ridge\": ridge_cv.coef_,\n",
    "    \"Lasso\": lasso_cv.coef_,\n",
    "    \"ElasticNet\": elastic_cv.coef_\n",
    "}, index=X_train_full.columns)\n",
    "\n",
    "coef_estimates"
   ],
   "id": "6ac9e196575fa506",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# PRS-only model R2\n",
    "r2_prs = r2_score(y_test, lr_baseline.predict(X_test_baseline))\n",
    "\n",
    "# Full model R2\n",
    "r2_full = r2_score(y_test, lr_full.predict(X_test_full))\n",
    "\n",
    "print(\"PRS-only R2:\", r2_prs)\n",
    "print(\"Full R2:\", r2_full)\n",
    "print(\"Environmental/demographic added:\", r2_full - r2_prs)"
   ],
   "id": "62c80893c61438c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(effects['beta'], kde=True, bins=40)\n",
    "plt.title(\"Distribution of True Variant Effect Sizes (β)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Proportion causal:\", effects['is_causal'].mean())"
   ],
   "id": "a647ddb1a0ecaa9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Because this dataset was fully simulated, the true effect sizes used to generate the phenotype are known. This allows a direct comparison between fitted model coefficients and the underlying generative model.\n",
    "\n",
    "Across all linear and shrinkage models, the estimated coefficients recover the correct hierarchy of predictor importance. The polygenic score has the largest effect (\\~0.656), environmental exposure shows a moderate effect (\\~0.345), sex shows a smaller but clearly nonzero effect (\\~0.186), and age has a very small effect (\\~0.009). This ordering matches the simulation design, where the true coefficients before standardization were β_PRS = 1.0, β_env = 0.5, β_sex = 0.3, and β_age ≈ 0.02.\n",
    "\n",
    "Variance decomposition also aligns with the simulation. The PRS-only model explains approximately 44% of the variance in the trait, reflecting the strong genetic component. Adding environmental and demographic predictors raises R² to ~0.56, consistent with their moderate true effects. The incremental gain of ~12% R² matches expectations given the noise and relative effect sizes.\n",
    "\n",
    "Finally, the distribution of true SNP effect sizes confirms a sparse genetic architecture: only about 5% of variants are causal, and most effect sizes are close to zero. This sparsity is indirectly reflected in the fitted models: Lasso and Elastic Net shrink the coefficient for age (the weakest predictor), while leaving the major predictors largely unchanged.\n",
    "\n",
    "Overall, the models accurately recover both the direction and relative magnitude of the true generative parameters, demonstrating that classical and penalized linear models perform reliably on this controlled polygenic simulation. This validates the simulation framework and provides a statistical foundation for the more complex ancestry inference methods that will be developed in the capstone project."
   ],
   "id": "e8cf23eac0dba0af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Bootstrap Confidence Intervals",
   "id": "9265978cf7401be1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def bootstrap_coefficients(X, y, n_boot=500):\n",
    "    coefs = []\n",
    "    n = len(y)\n",
    "    model = LinearRegression()\n",
    "\n",
    "    for _ in range(n_boot):\n",
    "        # sample with replacement\n",
    "        idx = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        X_boot = X.iloc[idx]\n",
    "        y_boot = y.iloc[idx]\n",
    "\n",
    "        model.fit(X_boot, y_boot)\n",
    "        coefs.append(model.coef_)\n",
    "\n",
    "    coefs = np.array(coefs)\n",
    "    return coefs"
   ],
   "id": "a8bf0fec45e3626",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "coef_samples = bootstrap_coefficients(X_train_full, y_train, n_boot=500)\n",
    "coef_samples.shape"
   ],
   "id": "6e7818c1a530210e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "coef_boot_df = pd.DataFrame(\n",
    "    coef_samples,\n",
    "    columns = X_train_full.columns\n",
    ")\n",
    "coef_boot_df"
   ],
   "id": "cb084a4b65706d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.violinplot(data=coef_boot_df, inner='box')\n",
    "plt.title(\"Bootstrap Distributions of Coefficients\")\n",
    "plt.ylabel(\"Coefficient Value\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "id": "a37a096d05f5433e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "coef_boot_df.plot(kind='density', subplots=True, layout=(2,2), figsize=(10,6), sharex=False)\n",
    "plt.suptitle(\"Bootstrap Density Plots for Each Coefficient\")\n",
    "plt.show()\n"
   ],
   "id": "e4ce79e5159d386d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "84a52698fed2b58c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cis = coef_boot_df.quantile([0.025, 0.975])\n",
    "cis"
   ],
   "id": "d72668550695162c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The bootstrap distributions provide a clear picture of coefficient stability in the simulated quantitative trait. The coefficient for `polygenic_score` shows a narrow, consistently positive distribution, reflecting its role as the strongest predictor in the generative model. The environmental exposure variable, `env_index`, also exhibits a tight and distinctly positive distribution, confirming it as the second most influential predictor. The coefficient for `sex` shows moderate variability but remains strictly positive across nearly all bootstrap samples, indicating a reliable but smaller effect.\n",
    "\n",
    "In contrast, the distribution of the `age` coefficient is centered extremely close to zero and exhibits the widest relative variability. Its 95% confidence interval likely spans a region around zero, reinforcing the conclusion that age contributes very little signal to the phenotype. This matches both the true simulation parameters (β_age ≈ 0.02) and the behavior observed in subset selection and shrinkage models.\n",
    "\n",
    "Overall, the bootstrap analysis confirms the stability and relative importance of the predictors: strong genetic influence (PRS), moderate environmental influence, a reliable but weaker effect of sex, and a negligible contribution from age. These results provide a robust, uncertainty-aware validation of the model’s ability to recover the true underlying genetic architecture."
   ],
   "id": "2098feee94db306a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PCA & Population Structure Analysis",
   "id": "3fe8349a2659d948"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if \"PC1\" in cohort.columns:\n",
    "    plt.figure(figsize=(5,4))\n",
    "    var = np.var(cohort[pcs], axis=0)\n",
    "    plt.bar(pcs, var)\n",
    "    plt.title(\"Variance Captured by PCs (not standardized PCA)\")\n",
    "    plt.ylabel(\"Variance\")\n",
    "    plt.show()"
   ],
   "id": "2e48a03f9447a58c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(cohort[\"PC1\"], cohort[\"PC2\"],\n",
    "            c=cohort[\"quant_trait\"], cmap=\"viridis\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PC1 vs PC2 Colored by Quantitative Trait\")\n",
    "plt.colorbar(label=\"quant_trait\")\n",
    "plt.show()"
   ],
   "id": "d0ac9a88a9d94afc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cohort[[\"quant_trait\"] + pcs].corr()",
   "id": "f5c8601374b300cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate full model without PCs\n",
    "full_no_pcs = evaluate_model(lr_full,\n",
    "                X_train_full, y_train,\n",
    "                X_test_full, y_test,\n",
    "                \"Full Model (No PCs)\")\n",
    "\n",
    "# Evaluate full model with PCs\n",
    "full_pcs = evaluate_model(lr_full_pca,\n",
    "                X_train_full_pca, y_train,\n",
    "                X_test_full_pca, y_test,\n",
    "                \"Full Model (With PCs)\")\n",
    "\n",
    "pd.DataFrame([full_no_pcs, full_pcs])"
   ],
   "id": "40b0cc59f69a8351",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Principal components (PC1 and PC2) were included in the simulated dataset to mimic a typical genomics pipeline where PCs often correct for population structure. However, because the data were generated under a single neutral population using msprime, little to no structure is expected.\n",
    "\n",
    "The PC1 vs PC2 scatterplot shows a single, diffuse cluster with no distinct ancestry groups and no visible gradient with respect to the quantitative trait. Correlations between the trait and the PCs are effectively zero, confirming that population structure does not influence trait variation in this simulation.\n",
    "\n",
    "Model performance further supports this conclusion. The full regression model with PCs achieved identical RMSE and R² values compared to the model without PCs, and the estimated coefficients changed only minimally. This behavior is expected when PCs represent neutral variation that is uncorrelated with the phenotype.\n",
    "\n",
    "These results highlight that PCA does not contribute to prediction or confounding correction in this dataset, which aligns with the known simulation design. This small experiment also provides a methodological bridge to the capstone project, where population structure and relatedness will play a more substantive role in genotype prediction and ancestral inference."
   ],
   "id": "39aab919a4227f38"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
